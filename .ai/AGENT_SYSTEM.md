# Agent System

## Agent Philosophy

AutoBE's agent system employs 40+ specialized AI agents that collaborate to transform requirements into executable code. Each agent is specialized for a specific domain with clear responsibilities and interfaces.

The core principle of agent design is **single responsibility**. Each agent performs one clear task. For example, Requirements Analyzer handles only requirement analysis, while Schema Generator handles only Prisma schema generation. This separation keeps each agent's System Prompt simple and makes testing and debugging easier.

Agents generate structured output through **Function Calling**. Rather than generating free-form text, they output according to predefined JSON schemas. This eliminates the need for parsing and guarantees type safety. If the LLM generates output outside the schema, it's automatically retried.

Agent communication happens through **events**. One agent's output becomes the next agent's input. For example, the analysis report generated by the Analyze agent is used as input for the Prisma agent, and the Prisma schema becomes input for the Interface agent. This pipeline structure creates clear data flow and enables independent testing of each stage.

## Agent Categories

AutoBE's agents are categorized by function.

**Planning Agents** handle plan creation. Scenario Agent determines which tasks to perform in what order. In the Analyze phase, it decides which documents to write; in the Interface phase, which APIs to generate; in the Test phase, which tests to write. Planning Agent output becomes the blueprint for subsequent agents.

**Generation Agents** create code or documents. Write Agent writes actual code, and Document Agent writes analysis reports. They have very detailed System Prompts and precisely follow coding conventions, naming rules, and architecture patterns. Generated code must be immediately compilable, requiring high accuracy.

**Review Agents** perform verification and improvement. Analyze Review Agent reviews written analysis reports and suggests improvements. Correct Agent analyzes and fixes compilation errors. Review Agents apply critical thinking - they actually find and solve problems rather than just approving.

**Specialized Agents** perform domain-specific tasks. Authorization Agent designs authentication/authorization logic, and ERD Agent generates entity relationship diagrams. They encode domain expertise in their System Prompts and perform specialized tasks that general agents cannot.

## Agent Lifecycle

Agent lifecycle follows clear stages.

**Initialization** stage prepares context needed by the agent. System Prompt, Tool definitions, and History are constructed, and the LLM client is initialized. Cache keys for Prompt Caching are also set at this stage.

**Execution** stage calls the LLM API to perform actual work. It requests structured output through Function Calling and receives responses via streaming. Progress events are emitted to provide real-time feedback to users.

**Validation** stage verifies agent output. It checks if Function Calling responses match the schema and all required fields exist. For Generation Agents, generated code is validated with compilers. On validation failure, retry or invoke Correct Agent.

**Completion** stage publishes results as events. Agent output is stored in state and becomes available for the next agent to reference. Completion events are sent to Frontend via WebSocket, updating the UI.

Agents have **idempotency**. They should always generate identical output for identical input. While complete idempotency is impossible due to LLM non-determinism, System Prompt design and Temperature settings maintain maximum consistency.

## Context Management

Agent performance heavily depends on context management.

**History Transformation** is key to context optimization. Raw history is very long with much duplication, but History Transformer extracts and reconstructs only the core information needed by the agent. For example, Realize Agent needs only Prisma schema and OpenAPI document, so the requirement analysis process is omitted.

**Prompt Caching Strategy** maximizes efficiency in repetitive tasks. When implementing 40 APIs in the Realize phase, Prisma schema and OpenAPI document are identical across all calls. These are placed as cacheable System Messages, with only API-specific content in User Messages. After the first call, cache hits dramatically reduce response time and cost.

**Progressive Context Loading** loads only necessary information step by step. Initial Planning phase provides only requirement summary, while actual Generation phase provides detailed specifications. This prevents unnecessary token usage and helps the LLM focus on core information.

**Context Window Management** handles ultra-large contexts. Claude 3.7 Sonnet supports 200K token context, but using it all is inefficient. Select only necessary information and leave the rest as references. For example, when implementing a specific API, provide only that OpenAPI Operation in detail, listing only paths and methods for other Operations.

## Error Handling

Agents must handle various error situations.

**LLM API Errors** occur due to network issues or rate limits. These transient errors are retried with exponential backoff strategy. Wait 1 second after first failure, 2 seconds after second failure, up to 3 retries maximum. Users are informed of progress even during retries.

**Schema Validation Errors** occur when Function Calling responses don't match the schema. Missing required fields, type mismatches, invalid enum values are causes. In this case, feed back the error to the LLM and request regeneration. Add emphasis to System Prompt to "follow schema exactly" to prevent recurrence.

**Compilation Errors** occur when generated code doesn't pass the compiler. Correct Agent intervenes to analyze compiler diagnostics and identify error location and cause. Generate corrected code and recompile. Most type errors are resolved in 1-2 iterations, with maximum retry count limited to prevent infinite loops.

**Logical Errors** occur when code compiles but is logically incorrect. For example, referencing non-existent tables or setting up wrong relationships. These cannot be detected by compiler alone - Review Agent or Test Agent discovers them. When found, improve that agent's System Prompt to prevent similar errors.

## Agent Communication Patterns

Agent communication follows clear patterns.

**Sequential Pipeline** is the most basic pattern. Agent A's output becomes Agent B's input, and B's output becomes C's input. The Analyze → Prisma → Interface → Test → Realize pipeline follows this pattern. Each stage waits for the previous stage to complete and executes sequentially.

**Parallel Fan-Out** is a pattern where multiple agents execute one plan in parallel. When 10 document writes are planned in Analyze phase, 10 Write Agents execute simultaneously. Each agent is independent and doesn't affect others' results. When all agents complete, proceed to next stage.

**Iterative Refinement** is a pattern of receiving feedback and iteratively improving. When Write Agent generates code, Correct Agent validates, and if errors exist, feeds back to Write Agent. This loop continues until compilation succeeds, with quality gradually improving.

**Hierarchical Delegation** is a pattern where upper agents delegate tasks to lower agents. Orchestrator manages overall flow and delegates actual work to specialized agents. For example, `orchestrateRealize` sequentially calls Authorization, Write, Correct agents and aggregates results.

## Agent Observability

Tracking and debugging agent behavior is essential.

**Event Logging** records all agent activities. Events like agent start, completion, error, retry are stored with timestamps. When issues occur, analyze event logs to identify which agent failed when.

**Progress Tracking** shows progress of long-running tasks. When implementing 40 APIs in Realize phase, update progress info like "completed: 15 / total: 40" in real-time. Users can predict completion time and confirm the system hasn't frozen.

**Performance Metrics** measure execution time and token usage for each agent. Identify which agents are bottlenecks and which stages consume many tokens to find optimization points. Can also quantitatively measure performance difference before and after applying Prompt Caching.

**Error Analytics** analyzes recurring error patterns. If a specific agent fails frequently, there may be issues with System Prompt or Tool definitions. Track frequency by error type to prioritize and continuously improve the system.

## Agent Evolution

The agent system continuously evolves.

**Prompt Iteration** is the process of improving System Prompts. Analyze user feedback, error logs, and generated code quality to update prompts. Make unclear instructions specific and add rules to prevent frequently occurring mistakes. Prompt changes are version controlled and improvement effects are validated through A/B testing.

**Tool Enhancement** is the process of improving Function Calling tools. Add new fields, define schemas more clearly, or add examples. If agents frequently output incorrect formats, strengthen schema constraints or add additional explanations to prompts.

**Architecture Refactoring** is the process of improving agent structure. If one agent has too many responsibilities, split into two; conversely, if too fragmented, consolidate. When new features are added, place agents in appropriate locations and integrate with existing pipeline.

**Performance Optimization** is the process of optimizing execution time and cost. Remove unnecessary context, expand Prompt Caching application, and find parallelizable parts for improvement. Use profiling to identify bottlenecks and focus on the most effective optimization points.

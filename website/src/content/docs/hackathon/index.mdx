import AutoBeWaterfallDiagram from "../../../template/diagrams/AutoBeWaterfallDiagram.mdx";

## 1. Overview

![](/images/hackathon/poster-20250912.png)

**Wrtn Technologies is hosting the 1st AutoBE Hackathon.**

### Hackathon Information

**Event Details**
- **Participants**: Maximum 70 people
- **Registration Period**: September 5 - 10, 2025
- **Event Schedule**: September 12 - 14, 2025 (64 hours)
  - **Start**: September 12, 08:00:00 (PDT, UTC-7)
  - **End**: September 14, 23:59:59 (PDT, UTC-7)
- **Winners Announcement**: September 17, 2025


#### We want to ask backend developers:
Can AI truly replace the work of backend developers? We seek to hear the answer to this question directly from developers working in the field.

AutoBE is an AI-based no-code platform that automatically generates backend applications through natural language conversations. When you discuss requirements with AutoBE's AI chatbot, AutoBE organizes them into a requirements specification, designs database schemas, defines APIs, writes test code, and ultimately implements a backend application that successfully builds. But is the generated code truly production-ready? Perhaps it's just seemingly plausible code fragments that differ from what users actually want?

This hackathon is designed precisely to validate this point. We expect developers with actual backend development experience to use AutoBE firsthand and provide honest, sharp evaluations from an expert perspective. Please tell us whether the backend applications generated by AutoBE truly match what you wanted. Your critical perspective and professional analysis will play a crucial role in developing AutoBE into a better tool.

## 2. What is AutoBE?
<details>
<summary>Descriptions</summary>
**AutoBE is a vibe coding agent for building backend applications, enhanced with AI-friendly compilers.**

- [Github Repository](https://github.com/wrtnlabs/autobe)
- [Guide Documents](https://autobe.dev/docs)

**Key Innovation**

AutoBE is an AI-based no-code backend generation platform that creates fully functional production-grade backend applications from natural language requirements alone.

To solve the fundamental limitation of existing AI code generation tools - that generated code often doesn't compile or run - we introduced an innovative approach called **Compiler-in-the-Loop**.

**Core Achievement**

A vibe coding agent exclusively for backend application generation with **100% build success rate** (based on OpenAI GPT 4.1) - that's AutoBE.

### 2.1. How It Works 

<AutoBeWaterfallDiagram />

AutoBE follows a 5-stage process that reinterprets the traditional software engineering waterfall model for the AI era. Each stage is handled by specialized AI agents, with compilers performing real-time validation at every stage.

The first stage, Analyze Agent, systematically analyzes requirements entered in natural language by users. Rather than simply listing features, it understands business logic, derives various user personas, and defines each user's permissions and roles. During this process, it identifies and clarifies ambiguous or conflicting requirements.

The second stage, Prisma Agent, designs database schemas based on requirements. It identifies relationships between entities, applies appropriate normalization, and establishes indexing strategies. Using Prisma ORM's schema definition language, it generates type-safe data models that are immediately validated by the Prisma compiler.

The third stage, Interface Agent, designs RESTful APIs. It defines HTTP methods, URIs, and request/response formats for each endpoint, generating complete API documentation according to OpenAPI 3.1 specifications. This documentation must pass the AutoBE-specific OpenAPI compiler before proceeding to the next stage.

The fourth stage, Test Agent, writes E2E test code. It creates test scenarios that simulate actual user behavior patterns, including not only normal cases but also edge cases and error situations. Generated test code must be executable and is validated by the test runner.

The final stage, Realize Agent, implements the actual backend code. Based on the NestJS framework, it implements controller, service, and repository layers, automatically handling advanced features like dependency injection, middleware, and guards. The final code must pass TypeScript compiler and NestJS builder.

### 2.2. Technical Features

AutoBE's most distinctive feature is the integration of specialized compilers at each stage. They validate in real-time whether AI-generated code is syntactically correct, type-consistent, and actually executable. When compilation errors occur, AI immediately receives feedback and modifies the code, repeating this process until complete code is generated.

AutoBE's core competitive advantage lies in the AI-specific compilers independently developed for Prisma, Interface, and Test domains. Unlike general development tools, these compilers deeply understand and are optimized for AI characteristics:

**AI-specific Prisma Compiler** goes beyond simply validating schema syntax to evaluate the logical consistency and relationship appropriateness of AI-designed data models. It preemptively detects and provides feedback on circular references or unnecessary duplicate relationships that AI tends to generate.

**AI-specific Interface Compiler** comprehensively validates not only OpenAPI 3.1 spec compliance but also RESTful principle adherence in API design, consistency between endpoints, and completeness of request/response structures. It automatically detects missing authentication headers or error response formats that AI often overlooks.

**AI-specific Test Compiler** analyzes whether generated test code performs meaningful validation beyond being merely executable. It evaluates test coverage, inclusion of edge cases, and realism of test scenarios to suggest improvement directions to AI.

The biggest differentiator of these compilers is how they communicate with AI. While regular compilers simply say "there's an error," AutoBE's compilers provide detailed feedback on "why it's a problem" and "how to fix it" in a way AI can understand. This close collaboration is the secret to 100% build success rate.

Another innovation of AutoBE is its structured code generation approach based on AST (Abstract Syntax Tree). After analyzing natural language requirements, AI generates data through function calling according to predefined AST structures. It's as if AI "assembles" code rather than "writes" it. The generated AST is validated by each compiler and finally converted into actual usable code.

You can check each compiler's AST structure on GitHub:
- **Prisma Compiler**: [`AutoBePrisma.IApplication`](https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/prisma/AutoBePrisma.ts)
- **Interface Compiler**: [`AutoBeOpenApi.IDocument`](https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/openapi/AutoBeOpenApi.ts) 
- **Test Compiler**: [`AutoBeTest.IFunction`](https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/test/AutoBeTest.ts)

This structured approach ensures consistency and quality of AI-generated code. It also provides a foundation for compilers to validate effectively, making AI and compilers work as a team.

Additionally, AutoBE uses a modern, proven technology stack including TypeScript, NestJS, Prisma ORM, and PostgreSQL/SQLite. This means generated code follows the same standards used in actual production environments.


### 2.3. Live Demonstration

We've prepared actual backend applications generated by AutoBE to prove its capabilities. These aren't prototypes or demos—they're fully functional production-grade applications created entirely through natural language conversations.

<br/>
<iframe
  src="https://www.youtube.com/embed/JNreQ0Rk94g"
  title="AutoBE Demonstration (Bullet-in Board System)"
  width="100%"
  style={{ aspectRatio: "16/9" }}
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerPolicy="strict-origin-when-cross-origin"
  allowFullScreen
></iframe>

**Example Applications Generated by AutoBE**

From simple todo applications to complex e-commerce platforms, AutoBE has successfully generated various types of backend systems. Each application includes:
- Properly structured databases
- RESTful APIs
- Comprehensive test suites
- Production-ready code following best practices

#### Generated Applications

**1. [Discussion Board](https://github.com/wrtnlabs/autobe-example-bbs)**

**2. [To Do List](https://github.com/wrtnlabs/autobe-example-todo)**

**3. [Reddit Community](https://github.com/wrtnlabs/autobe-example-reddit)**

**4. [E-Commerce Platform](https://github.com/wrtnlabs/autobe-example-shopping)**
   - [Requirements Analysis Report](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/docs/analysis)
   - [Entity Relationship Diagram](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/docs/ERD.md) / [Prisma Schema](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/prisma/schema)
   - [API Controllers](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/src/controllers) / [DTO Structures](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/src/api/structures)
   - [E2E Test Functions](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/test/features/api)
   - [API Implementations](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/src/providers)
   - [AI Review](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/AI_REVIEW.md)


#### How Simple Is It?

The process is remarkably simple. Creating a discussion board with AutoBE requires just five natural language commands. No coding knowledge, no technical jargon—just describe what you want:

**Example: Creating a Discussion Board**

1. I want to create a political/economic discussion board. Since I'm not familiar with programming, please write a requirements analysis report as you see fit.
2. Design the database schema.
3. Create the API interface specification.
4. Make the e2e test functions.
5. Implement API functions.

That's it. In about 70 minutes, you'll have a complete backend application ready to deploy. This isn't theory—it's how we generated all the examples above.

> [!TIP] 
>
> **For Hackathon Participants**
>
> Yes, these demo prompts are ridiculously simple. But during the hackathon, please don't just say "do everything by yourself!" 
>
> Actually discuss your requirements in detail with the AI. The better your input, the better your output will be.
</details>

## 3. Purpose of the Hackathon
<details>
<summary>Descriptions</summary>

AutoBE seems theoretically perfect. It has all the elements: systematic processes, compiler validation, modern technology stack. Generated code actually compiles, runs, and passes tests. But we still don't have an answer to one important question.

Is the backend application generated by AutoBE really what users wanted?

Until now, our development team has focused on whether each component of AutoBE works correctly. We've tested whether compilers perform accurate validation, whether agents generate appropriate code, and whether the entire system operates stably. But these were all validations from a technical perspective.

In actual development fields, there are things as important as technical completeness. Is the generated code easy to maintain? Is the architecture scalable? Is performance optimization appropriate? Are there security vulnerabilities? Above all, is it "good code" from a developer's perspective?

To answer these questions, we need evaluations from experts with actual backend development experience. While automated code review through AI is possible, we trust insights from human intuition and experience more. Especially evaluations from the perspective of "What if I had to take over this code?" can only be done by actual developers.

### 3.1. What We Want to Hear

We want specific, practical feedback from you, not simple praise or criticism. We want to know how AutoBE's generated requirements specifications compare to those used in actual projects, whether database design is reasonable long-term, whether API design properly follows RESTful principles, whether test code performs meaningful validation, and whether the final implementation code has production-level quality.

We're also curious about how AutoBE's generated code differs from what you would write yourself. What's better, what's lacking, and what direction should we take for improvement?

Above all, we expect your honest evaluation on whether AutoBE is truly a tool that can improve developer productivity or merely an interesting technical demo.
</details>

## 4. Eligibility and Requirements
<details>
<summary>Descriptions</summary>
### Who We're Looking For

**1. Backend Development Experience**
- At least **1 year** of practical experience
- Not just learning, but actually developing and operating real services

**2. Technology Stack Experience**

At least one of the following:
- Node.js (Express or NestJS)
- Java (Spring Boot)
- Python (Django or FastAPI)
- Similar backend frameworks

**3. Database Skills**
- Relational database design experience
- Beyond simple CRUD operations
- Experience with:
  - Designing relationships between tables
  - Establishing indexing strategies
  - Query optimization

**4. API Design Knowledge**
- Understanding of RESTful principles
- Experience applying them in real projects

### Language Requirements

**English Proficiency is Essential**
- All conversations with AutoBE are in English
- All generated code and documentation are in English
- You need:
  - Conversational ability to communicate naturally
  - Reading comprehension for technical documentation


### Technical Requirements

- Personal laptop or desktop computer
- Ability to download and run generated code locally
- Recommended to have installed:
  - Node.js
  - Git
  - Your preferred code editor
</details>


## 5. How to Participate
<details>
<summary>Descriptions</summary>

### 5.1. Registration

https://forms.gle/8meMGEgKHTiQTrCT7

Those who wish to participate in the hackathon should submit an application through Google Forms. The application requires basic personal information along with information about your backend development experience.

This hackathon is limited to **70 participants on a first-come, first-served basis** and will close early if 70 people register before the deadline of September 10, 2025. The first 70 eligible applicants can participate.

The application deadline is September 10, 2025, and applications will not be accepted after that. Confirmed participants will be notified individually via email on September 11.

### 5.2. Account Issuance and Preparation

On September 12, the day before the event, detailed participation instructions will be sent to participants' individual emails. This email will include unique IDs and passwords to access the AutoBE platform, along with available AI model information.

We'll also provide a simple guide on how to use AutoBE and contact information for technical assistance during the hackathon. We recommend preparing your local development environment in advance if possible. It's good to have Node.js, Git, and your preferred code editor installed.

### 5.3. Hackathon Process

The hackathon starts at 8:00 AM PDT on September 12. Participants log into the AutoBE platform with provided accounts and must generate a total of 2 backend applications using two different AI models (`openai/gpt-4.1-mini` and `openai/gpt-4.1`).

When using each model, you must create applications with different themes. For example, try a simple todo app for the first model and a more complex e-commerce platform for the second. This is to evaluate each model's capabilities and limitations from various perspectives.

During generation, please carefully record conversation content with AutoBE, results from each stage, problems encountered and solutions. Taking screenshots or saving logs is also good. These materials will be important evidence when writing reviews later.

### 5.4. Submission

During the hackathon period (by 23:59:59 PDT on September 14, 2025), you must write and submit detailed review documents. **Important: You must submit a separate review for each generated backend application.** For example, if you generate 2 applications (one with `gpt-4.1-mini` and one with `gpt-4.1`), you must write 2 individual reviews. Reviews should be posted to AutoBE's GitHub Discussions at https://github.com/wrtnlabs/autobe/discussions/categories/hackathon-20250912.

Review documents have no specific format requirements but should include sufficiently detailed analysis for each project. Please explain not just "good" or "bad" but specifically what parts were good or bad in what ways and why. Do not combine multiple applications into one review - each application deserves its own thorough evaluation.
</details>


## 6. Provided AI Models
<details>
<summary>Descriptions</summary>

### 6.1. `openai/gpt-4.1-mini`

![](/images/demonstrate/replay-openai-gpt4.1-mini.png)

This model is a practical choice balancing performance and cost. While large-scale applications are somewhat challenging, it has sufficient capability to generate small to medium-sized backend applications. It's particularly suitable for systems with about 20 tables and around 150 API endpoints.

It shows excellent performance generating backends for common web services like community boards, blog platforms, and project management tools. It implements not only basic CRUD operations but also common features like user authentication, permission management, and file uploads well. Particularly noteworthy is its strength in the early stages of development—requirements analysis and API design. The model excels at understanding your natural language requirements and converting them into well-structured specifications and clean API designs. These outputs are consistently high-quality, making it an excellent choice for project initialization.

However, there are some limitations. It occasionally makes logical errors when implementing complex business logic or fails to completely resolve compilation errors when generating E2E test code. This isn't due to technical defects in AutoBE itself, but rather the inherent limitations of this lightweight model.

We deliberately provide this model first to demonstrate the importance of model capacity in AI-powered code generation—and to be frank, using only the most powerful models from the start would make hosting this hackathon financially unfeasible. The cost difference between models is substantial, which you'll understand when you later access the more powerful `openai/gpt-4.1`. 

Despite these limitations, `gpt-4.1-mini` still generates practical-level code that helps you understand AutoBE's capabilities. In fact, many developers find a cost-effective workflow by using this model for initial project generation, then refining the code with AI assistants like Claude Code or GitHub Copilot. This hybrid approach leverages the strengths of AutoBE's structured generation while maintaining budget efficiency—a pragmatic solution for real-world development.

### 6.2. `openai/gpt-4.1`

![](/images/demonstrate/replay-openai-gpt4.1.png)

> Available only after completing `openai/gpt-4.1-mini` review
>
> Once you complete your review of the mini model, you'll immediately gain access to the full-power `openai/gpt-4.1`

This is the most powerful AI model currently available, optimized for generating large-scale enterprise-grade backend applications. It can understand and implement complex business logic and handles large systems with over 500 API endpoints and over 1,000 test scenarios without problems.

This model's strength lies in context understanding. It grasps subtle connections between requirements and can infer implicit requirements. It's also proficient in implementing advanced features, automatically implementing real-time notification systems, complex permission systems, transaction processing, and caching strategies.

Here's where the real magic happens: with this model, AutoBE achieves a true 100% build success rate. Every single backend application generated with `openai/gpt-4.1` compiles perfectly, passes all tests, and is genuinely production-ready. The difference from the mini model is night and day—all those compilation errors in test and realize stages? They simply don't exist here.

But this powerful performance comes at a steep cost. Generating a typical e-commerce platform consumes about 150 million tokens, equivalent to about $300-400. Due to these high costs, we cannot accept unlimited hackathon participants and must carefully manage access to this premium model. 

That's why we require you to first experience and review `openai/gpt-4.1-mini`—not only does this help us manage costs, but it also gives you valuable perspective on how model capacity impacts code generation quality. Fortunately, once you unlock access, it's provided completely free to hackathon participants, so you can use it freely without any cost concerns.

### 6.3. `qwen/qwen3-235b-a22b-202507`

![](/images/demonstrate/replay-qwen3-235b-a22b.png)

> Optional - Just for Fun!
>
> This model is NOT required for the hackathon. It's included purely for fun and for those curious about local LLM performance!

This is the lightest open-source based model, requiring only laptop-level resources. We've included it as a bonus for participants who are curious about how local LLMs perform in code generation tasks compared to commercial cloud models. Think of it as a playground to explore the current state of open-source AI models.

Due to input token limitations, it can only generate small-scale applications but performs sufficiently for simple projects. It's suitable for generating applications with 5-10 tables and around 20 API endpoints, such as todo apps, memo applications, and simple accounting books. It implements basic CRUD operations and simple business logic without difficulty.

However, this model has significant limitations. It struggles to understand complex requirements and often fails to resolve compilation errors, causing process interruptions. But that's exactly what makes it interesting! If you're curious about the performance gap between local open-source models and commercial cloud models, this is your chance to experience it firsthand. Who knows? You might be surprised by what it can (or can't) do.
</details>


## 7. Evaluation Criteria and Review Writing Guide
<details>
<summary>Descriptions</summary>

### 7.1. Requirements Analysis Stage Evaluation


**1. Understanding & Documentation**
- How accurately were your natural language requirements understood?
- Are relationships and priorities between features clearly defined?
- Is it more than just a feature list?

**2. User Personas & Roles**
- Are various user types considered?
- Are permissions and accessible features logically designed?
- Is the role hierarchy appropriate?

**3. Non-functional Requirements**
- Performance considerations
- Security measures
- Scalability planning

**4. Document Quality**
- Easy to read and understand?
- Any ambiguous or conflicting content?
- Sufficient detail to start development?


### 7.2. Database Design Evaluation

**1. Production-Readiness**
- Are table relationships logically valid?
- Any unnecessary duplications?
- Any circular references?

**2. Normalization Level**
- Over-normalized (complex joins)?
- Under-normalized (data integrity issues)?
- Appropriate balance?

**3. Keys & Indexing**
- Primary/foreign keys correctly set?
- Indexing strategy considers query performance?
- Missing indexes for common queries?

**4. Technical Details**
- Naming convention consistency
- Appropriate data types
- Default values and constraints
- Scalability for future changes

### 7.3. API Design Evaluation

**1. RESTful Principles**
- HTTP methods used meaningfully?
- URIs resource-centric?
- Status codes properly utilized?

**2. API Consistency**
- Similar endpoints follow patterns?
- Request/response formats unified?
- Error responses standardized?
- Common features (pagination, filtering, sorting) consistent?

**3. Documentation Quality**
- OpenAPI specs complete?
- Parameter descriptions sufficient?
- Clear examples provided?

**4. Security & Auth**
- Authentication/authorization reasonable?
- Sensitive data protected?
- API key management appropriate?

### 7.4. Test Code Evaluation

**1. Meaningful Validation**
- Verifies business logic, not just API calls?
- Tests actual functionality?
- Appropriate assertions?

**2. Scenario Completeness**
- Normal use cases covered?
- Exception situations tested?
- Edge cases included?
- User behavior patterns reflected?
- Important user journeys tested?

**3. Code Quality**
- Clear test function names?
- Appropriate test data setup?
- Specific assertions?
- Test independence guaranteed?
- Easy to debug when failing?

### 7.5. Implementation Code Evaluation

**1. Code Quality**
- Readable and understandable?
- Appropriate abstraction?
- Follows SOLID principles?
- Proper modularization?

**2. Architecture**
- Clear layer separation?
- Dependency injection utilized?
- Easy to extend/modify?
- Error handling systematic?
- Logging at appropriate levels?

**3. Performance**
- Efficient database queries?
- N+1 problems?
- Caching strategies?
- Resource optimization?

**4. Security & Types**
- SQL injection vulnerabilities?
- Input validation?
- TypeScript types properly used?
- No `any` type abuse?

### 7.6. Overall Evaluation

**Big Picture Questions**

**1. AutoBE Assessment**
- Overall strengths and weaknesses?
- Suitable project types?
- Unsuitable project types?
- Best use cases in actual development?

**2. Practical Impact**
- Actual development time reduction?
- Code quality level (junior/mid/senior)?
- Could you maintain this code?
- Would you use it in production?

**3. Improvement Suggestions**
- Specific parts to improve
- How to improve them
- Priority order
- Missing features

> **Remember**
>
> Don't just say "I wish it generated better code."
> Be specific about what should be improved and how.
</details>
## 8. Prizes and Benefits
<details>
<summary>Descriptions</summary>
### 8.1. Grand Prize (1 person)

The person who writes the best review will receive $2,000. We'll select reviews that evaluate AutoBE from a professional, balanced perspective and present specific, actionable improvement suggestions, not simply reviews with more volume or praise.

The grand prize winner will receive $2,000.

### 8.2. Excellence Award (1 person)

The person who writes the second-best review will receive $1,000. The excellence award will also be selected based on review professionalism and insights.

### 8.3. Participation Prize (All who meet evaluation criteria)

All who participate sincerely and provide meaningful feedback will receive a $50 participation prize. However, all following conditions must be met:
- Generate projects using both required AI models (`openai/gpt-4.1-mini` and `openai/gpt-4.1`)
- Write detailed reviews for each project
- Include all required evaluation elements
- Meet minimum content requirements

### 8.4. Exclusion Conditions

Participation prizes will not be paid in the following cases:
- Not providing even minimal review feedback
- Writing reviews using AI as proxy
- Not using both required models
- Writing perfunctory or insincere reviews
- Plagiarizing others' reviews

AI-assisted review writing is not allowed. The core purpose of this hackathon is to collect genuine feedback based on actual backend developers' experience. AutoBE's development requires vivid opinions about inconveniences, improvements, and utilization possibilities felt in practice. Formal reviews generated by AI don't serve this purpose, so participants found doing so will be excluded from evaluation.

### 8.5. Judging and Announcement

Results will be announced via individual email and official website (https://autobe.dev) on **SEPTEMBER 17th 2025**. Judging will be conducted jointly by the AutoBE development team and external experts, comprehensively evaluating review professionalism, specificity, practicality, and balance.

Prizes will be paid within one week after results announcement, and you must submit account information capable of international transfers. Tax issues must be handled directly by recipients, and we'll provide necessary documents.
</details>

## 9. Disclaimer
<details>
<summary>Descriptions</summary>

### 9.1. Beta Version Limitations

AutoBE is currently in beta, still in pre-release development stage. Therefore it's not perfect and may have various problems and limitations. These are characteristics of the current development state, not bugs, so please understand and participate.

Generated code may not always be optimized and can sometimes be inefficient or unnecessarily complex. Also, compilation or runtime errors may occur in certain situations, and the process may stop without resolving them.

### 9.2. Use of Generated Code

We don't recommend using hackathon-generated code in actual production environments. Code generated by AutoBE hasn't undergone sufficient validation and may contain security vulnerabilities or performance issues.

If you decide to actually use generated code, please use it only after professional code review and security audit. Wrtn Technologies is not responsible for any issues arising from using AutoBE-generated code.

### 9.3. Open Source and Public Review Notice

AutoBE is an open-source project, and all hackathon reviews will be publicly posted on GitHub Discussions. Therefore, when using the AI chatbot during the hackathon, please be extremely careful not to input any sensitive personal information or business confidential information. Everything you discuss with the AI and all generated code will be part of your public review.

Remember: Your conversations, generated applications, and reviews will be visible to anyone on the internet. Plan your hackathon projects accordingly and avoid using real business ideas or proprietary information.
</details>
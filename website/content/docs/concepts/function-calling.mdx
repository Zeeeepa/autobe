---
title: AutoBE > Guide Documents > Concepts > AI Function Calling
---
import { Tabs } from "nextra/components";

import RemoteSource from "../../../components/RemoteSource";

## AI Function Calling

<Tabs items={[
  <code>IAutoBePrismaValidation</code>,
  <code>AutoBePrisma.IApplication</code>,
  <code>AutoBeOpenApi.IDocument</code>,
]}>
  <Tabs.Tab>
    <RemoteSource 
      url="https://raw.githubusercontent.com/wrtnlabs/autobe/refs/heads/main/packages/interface/src/prisma/IAutoBePrismaValidation.ts" filename="@autobe/interface" 
      showLineNumbers />
  </Tabs.Tab>
  <Tabs.Tab>
    <RemoteSource 
      url="https://raw.githubusercontent.com/wrtnlabs/autobe/refs/heads/main/packages/interface/src/prisma/AutoBePrisma.ts" 
      filename="@autobe/interface" 
      showLineNumbers />
  </Tabs.Tab>
  <Tabs.Tab>
    <RemoteSource 
      url="https://raw.githubusercontent.com/wrtnlabs/autobe/refs/heads/main/packages/interface/src/openapi/AutoBeOpenApi.ts" 
      filename="@autobe/interface" 
      showLineNumbers />
  </Tabs.Tab>
</Tabs>

`@autobe` fundamentally prefers AI function calling to generate AST (Abstract Syntax Tree) data over having AI write raw programming code as text. The system validates the AST data generated through AI function calling, provides feedback to the AI for correction when errors are detected, and finally converts the validated AST data into actual programming code.

Therefore, the success of `@autobe` depends on three critical factors. First, how precisely and efficiently we can create AI function calling schemas. Second, how detailed and accurate validation feedback we can provide to the AI when it generates incorrect AST data. Finally, how clearly we can communicate the coding rules that `@autobe` must follow while constructing code (composing AST data) to the AI.

To address these challenges, `@autobe` adopts [`typia`](https://github.com/samchon/typia) to generate AI function calling schemas at the compiler level. The compiler integrated with [`typia`](https://github.com/samchon/typia) not only creates AI function calling schemas but also generates validation functions for each type. Moreover, the coding rules that `@autobe` must follow are embedded as comments in each AST type. These comments are recorded as type descriptions when [`typia`](https://github.com/samchon/typia) converts TypeScript types into AI function calling schemas.

This approach creates a comprehensive development story that ensures both type safety and rule compliance through compiler-driven automation.

## Schema Made by Compiler

<Tabs items={[
  "Schema Generation",
  <code>AutoBePrisma.IApplication</code>,
  <code>AutoBeOpenApi.IDocument</code>
]}>
  <Tabs.Tab>
```typescript
import { AutoBeOpenApi, AutoBePrisma } from "@autobe/prisma";
import { ILlmApplication } from "@samchon/openapi";
import typia from "typia";

const app: ILlmApplication<"chatgpt"> = typia.llm.application<
  ICodeGenerator,
  "chatgpt",
  { reference: true }
>();
console.log(app);

interface ICodeGenerator {
  /**
   * Generate Prisma AST application for database schema design. 
   */
  generatePrismaSchema(app: AutoBePrisma.IApplication): void;

  /**
   * Generate OpenAPI document for RESTful API design.
   */
  generateOpenApiDocument(app: AutoBeOpenApi.IDocument): void;
}
```
  </Tabs.Tab>
  <Tabs.Tab>
     <RemoteSource
        url="https://raw.githubusercontent.com/wrtnlabs/autobe/main/packages/interface/src/prisma/AutoBePrisma.ts"
        filename="@autobe/interface"
        showLineNumbers />
  </Tabs.Tab>
  <Tabs.Tab>
     <RemoteSource
        url="https://raw.githubusercontent.com/wrtnlabs/autobe/main/packages/interface/src/openapi/AutoBeOpenApi.ts"
        filename="@autobe/interface"
        showLineNumbers />
  </Tabs.Tab>
</Tabs>

> [ðŸ’» Typia Playground Link](https://typia.io/playground/?script=JYWwDg9gTgLgBAbzgeTAUwHYEEzDgXzgDMoIQ4AiAAQGcBDEAYwAsIMB6CdDO3CgbgBQoSLDgwAnrjrFS5CpOkDBwjDDRQidRmjgBJAOKYNdGNESC4cdgCoblqzbhGMJ9XDoYU3LAAU9cAAmEIwAriCY8ETmAEpoNDBEoQA2cH4BgfHAAOYYwBjZAHQOcDbsDtnGUKZoABRgAFwWVlbBYRFqTaiYOMCFegAiIeGRQlb4AJRNAG4QwIFC+CqKwHSFyckghbxgycCMpsBsADwOhlWm0AA0DhQsptlgMBQ3VkhQaEQamDpNMFChXRLAB8tQmQiAA)

AST (Abstract Syntax Tree) structures are inherently recursive with infinite depth and countless union types. Therefore, creating AI function calling schemas for AST should never be done manually. It must be automatically generated by a compiler to ensure both stability and productivity.

The [`typia.llm.application<App, Model, Options>()`](https://typia.io/docs/llm/application/) function serves as the cornerstone of this approach. This powerful utility automatically generates comprehensive AI function calling schemas from TypeScript type definitions. By leveraging compile-time type analysis, it creates schemas that are not only type-safe but also include rich metadata and validation rules embedded directly in the type system.

The function supports multiple AI models through its generic parameters, allowing for model-specific optimizations and feature support. The options parameter enables fine-tuning of schema generation, including reference handling, description enrichment, and validation strictness. This compiler-driven approach eliminates the manual effort and potential errors associated with hand-crafted schemas while providing superior type safety and maintainability.

## Comments over Prompts

```typescript filename="@autobe/interface" showLineNumbers
export namespace AutoBePrisma {
  /**
   * Interface representing a single Prisma schema file within the application.
   *
   * Each file focuses on a specific business domain and contains related
   * models. File organization follows domain-driven design principles as seen
   * in the uploaded schemas.
   */
  export interface IFile {
    /**
     * Name of the schema file to be generated.
     *
     * Should follow the naming convention: "schema-{number}-{domain}.prisma"
     * Examples: "schema-02-systematic.prisma", "schema-03-actors.prisma" The
     * number indicates the dependency order for schema generation.
     */
    filename: string & tags.Pattern<"^[a-zA-Z0-9._-]+\\.prisma$">;

    /**
     * Business domain namespace that groups related models.
     *
     * Used in Prisma documentation comments as "@\namespace directive".
     * Examples from uploaded schemas: "Systematic", "Actors", "Sales", "Carts",
     * "Orders", "Coupons", "Coins", "Inquiries", "Favorites", "Articles"
     */
    namespace: string;

    /**
     * Array of Prisma models (database tables) within this domain.
     *
     * Each model represents a business entity or concept within the namespace.
     * Models can reference each other through foreign key relationships.
     */
    models: IModel[];
  }

  /**
   * Interface representing a single Prisma model (database table).
   *
   * Based on the uploaded schemas, models follow specific patterns:
   *
   * - Main business entities (e.g., shopping_sales, shopping_customers)
   * - Snapshot/versioning entities for audit trails (e.g.,
   *   shopping_sale_snapshots)
   * - Junction tables for M:N relationships (e.g.,
   *   shopping_cart_commodity_stocks)
   * - Materialized views for performance (prefixed with mv_)
   */
  export interface IModel {
    /**
     * Name of the Prisma model (database table name).
     *
     * Should follow snake_case convention with domain prefix. Examples:
     * "shopping_customers", "shopping_sale_snapshots", "bbs_articles"
     * Materialized views use "mv_" prefix: "mv_shopping_sale_last_snapshots"
     */
    name: string & tags.Pattern<"^[a-z][a-z0-9_]*$">;

    /**
     * Detailed description explaining the business purpose and usage of the
     * model.
     *
     * Should include:
     *
     * - Business context and purpose
     * - Key relationships with other models
     * - Important behavioral notes or constraints
     * - References to related entities using "{@\link ModelName}" syntax Example:
     *   "Customer information, but not a person but a **connection** basis..."
     */
    description: string;

    /**
     * Indicates whether this model represents a materialized view for
     * performance optimization.
     *
     * Materialized views are read-only computed tables that cache complex query
     * results. They're marked as "@\hidden" in documentation and prefixed with
     * "mv_" in naming. Examples: mv_shopping_sale_last_snapshots,
     * mv_shopping_cart_commodity_prices
     */
    material: boolean;

    //----
    // FIELDS
    //----
    /**
     * The primary key field of the model.
     *
     * In all uploaded schemas, primary keys are always UUID type with "@\id"
     * directive. Usually named "id" and marked with "@\db.Uuid" for PostgreSQL
     * mapping.
     */
    primaryField: IPrimaryField;

    /**
     * Array of foreign key fields that reference other models.
     *
     * These establish relationships between models and include Prisma relation
     * directives. Can be nullable (optional relationships) or required
     * (mandatory relationships). May have unique constraints for 1:1
     * relationships.
     */
    foreignFields: IForeignField[];

    /**
     * Array of regular data fields that don't reference other models.
     *
     * Include business data like names, descriptions, timestamps, flags,
     * amounts, etc. Common patterns: created_at, updated_at, deleted_at for
     * soft deletion and auditing.
     */
    plainFields: IPlainField[];

    //----
    // INDEXES
    //----
    /**
     * Array of unique indexes for enforcing data integrity constraints.
     *
     * Ensure uniqueness across single or multiple columns. Examples: unique
     * email addresses, unique codes within a channel, unique combinations like
     * (channel_id, nickname).
     */
    uniqueIndexes: IUniqueIndex[];

    /**
     * Array of regular indexes for query performance optimization.
     *
     * Speed up common query patterns like filtering by foreign keys, date
     * ranges, or frequently searched fields. Examples: indexes on created_at,
     * foreign key fields, search fields.
     */
    plainIndexes: IPlainIndex[];

    /**
     * Array of GIN (Generalized Inverted Index) indexes for full-text search.
     *
     * Used specifically for PostgreSQL text search capabilities using trigram
     * operations. Applied to text fields that need fuzzy matching or partial
     * text search. Examples: searching names, nicknames, titles, content
     * bodies.
     */
    ginIndexes: IGinIndex[];
  }
}
```

`@autobe` invests significantly more effort in embedding coding rules as comments within the types used for AI function calling rather than relying on system prompts to enforce these rules.

This approach stems from a critical observation: AI models often fail to consistently follow system prompt rules, and these rules are frequently ignored entirely when context grows large. In contrast, description information written in function calling schemas is reliably followed by AI models. When AI generates information for a type specified through function calling, it references the type's description information again, making type-level comments far more effective than system prompts in coding agents.

The [`AutoBePrisma.IFile`](/api/interfaces/_autobe_interface.AutoBePrisma.IFile.html) and [`AutoBePrisma.IModel`](/api/interfaces/_autobe_interface.AutoBePrisma.IModel.html) types exemplify this approach by meticulously documenting the rules that must be followed for each interface and property type. For example, the `filename` property must follow the `schema-{number}-{domain}.prisma` format, and when defining database models, the name property must use snake_case convention with domain prefixes like `shopping_customers` or `bbs_articles`. Each comment provides concrete examples and specific constraints that guide the AI toward generating compliant code structures.

This comment-driven approach ensures that coding rules are embedded directly where the AI encounters them during code generation, creating a more reliable and maintainable system than traditional prompt-based instruction methods.

## Validation Feedback

<Tabs 
  items={[
    <code>ILlmApplication</code>,
    <code>ILlmFunction</code>,
    <code>IValidation</code>,
  ]} 
  defaultIndex={1}>
  <Tabs.Tab>
    <RemoteSource 
      url="https://raw.githubusercontent.com/samchon/openapi/refs/heads/master/src/structures/ILlmApplication.ts" 
      filename="@samchon/openapi" 
      showLineNumbers />
  </Tabs.Tab>
  <Tabs.Tab>
    <RemoteSource 
      url="https://raw.githubusercontent.com/samchon/openapi/refs/heads/master/src/structures/ILlmFunction.ts" 
      filename="@samchon/openapi" 
      showLineNumbers
      highlight="84-113" />
  </Tabs.Tab>
  <Tabs.Tab>
    <RemoteSource 
      url="https://raw.githubusercontent.com/samchon/openapi/refs/heads/master/src/structures/IValidation.ts" 
      filename="@samchon/openapi" 
      showLineNumbers />
  </Tabs.Tab>
</Tabs>

Does AI function calling always produce type-valid AST data? The answer is no. AI frequently experiences hallucinations even during function calling, generating AST data that is not type-valid. To correct these AI errors, validation feedback must be provided back to the AI.

The [`typia.llm.application<App, Model, Options>()`](https://typia.io/docs/llm/application/) function used by `@autobe` to create AI function calling schemas for AST data includes a built-in runtime type checker specialized for AI validation feedback. This validator doesn't just identify type violations; it generates detailed, AI-friendly error messages that help the model understand exactly what went wrong and how to fix it.

Even when AI function calling produces AST data that is type-correct, there are cases where the data fails to follow `@autobe`'s coding rules or contains contradictions that make conversion to programming code impossible. In these scenarios, `@autobe`'s custom-developed compiler's built-in compilation feedback feature activates to assist in AI correction.

This dual-layer validation system ensures that both type safety and business rule compliance are maintained throughout the code generation process. The compiler-level feedback provides semantic validation beyond simple type checking, catching logical inconsistencies and rule violations that could lead to non-functional generated code.

By combining runtime type validation with semantic rule validation, `@autobe` creates a robust feedback loop that continuously improves AI output quality while maintaining the integrity of the generated codebase. This approach transforms potential AI hallucinations from fatal errors into learning opportunities, allowing the system to iteratively refine its output until it meets all requirements.